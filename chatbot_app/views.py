# ai_chatbot_project/chatbot_app/views.py
import json
import requests
import os
import faiss
import numpy as np
import pickle
from django.shortcuts import render
from django.http import JsonResponse, StreamingHttpResponse
from django.views.decorators.csrf import csrf_exempt # CSRF ë³´í˜¸ ë¹„í™œì„±í™” (ê°œë°œìš©)
from sentence_transformers import SentenceTransformer

from .models import FAQ, FAQCategory

# --- RAG ê´€ë ¨ ì „ì—­ ë³€ìˆ˜ ë° í•¨ìˆ˜ ---
# ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (FastAPI ì„œë²„ì™€ ë™ì¼í•œ ëª¨ë¸ì´ì–´ì•¼ í•¨)
# Django ì•±ì´ ì‹œì‘ë  ë•Œ í•œ ë²ˆë§Œ ë¡œë“œë˜ë„ë¡ í•©ë‹ˆë‹¤.
print("Django ì›¹ ì„œë²„: ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
try:
    EMBEDDING_MODEL = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')
    print("Django ì›¹ ì„œë²„: ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")
except Exception as e:
    print(f"Django ì›¹ ì„œë²„: ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    EMBEDDING_MODEL = None # ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œ Noneìœ¼ë¡œ ì„¤ì •

# ë²¡í„° DBê°€ ì €ì¥ëœ ë””ë ‰í† ë¦¬ (scripts/build_faq_vector_db.pyì™€ ë™ì¼í•œ ê²½ë¡œ)
VECTOR_DB_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), '../vector_dbs')

# ë¡œë“œëœ FAISS ì¸ë±ìŠ¤ì™€ ID ë§¤í•‘ì„ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
# ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì¸ë±ìŠ¤ë¥¼ í•œ ë²ˆë§Œ ë¡œë“œí•˜ë„ë¡ ìºì‹±í•©ë‹ˆë‹¤.
LOADED_VECTOR_DBS = {}

def load_vector_db_for_category(category_name):
    """íŠ¹ì • ì¹´í…Œê³ ë¦¬ì— ëŒ€í•œ FAISS ì¸ë±ìŠ¤ì™€ ID ë§¤í•‘ ì •ë³´ë¥¼ ë¡œë“œí•˜ê±°ë‚˜ ìºì‹œëœ ê²ƒì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if category_name in LOADED_VECTOR_DBS:
        return LOADED_VECTOR_DBS[category_name]['index'], LOADED_VECTOR_DBS[category_name]['faq_ids']

    index_file_path = os.path.join(VECTOR_DB_DIR, f'faiss_index_{category_name}.bin')
    id_mapping_file_path = os.path.join(VECTOR_DB_DIR, f'id_mapping_{category_name}.pkl')

    if not os.path.exists(index_file_path) or not os.path.exists(id_mapping_file_path):
        print(f"ì˜¤ë¥˜: '{category_name}'ì— ëŒ€í•œ ë²¡í„° DB íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return None, None

    print(f"'{category_name}' ë²¡í„° DB ë¡œë“œ ì¤‘...")
    try:
        index = faiss.read_index(index_file_path)
        with open(id_mapping_file_path, 'rb') as f:
            faq_ids = pickle.load(f)

        LOADED_VECTOR_DBS[category_name] = {'index': index, 'faq_ids': faq_ids}
        print(f"'{category_name}' ë²¡í„° DB ë¡œë“œ ì™„ë£Œ. ì´ {index.ntotal}ê°œ ë²¡í„°.")
        return index, faq_ids
    except Exception as e:
        print(f"'{category_name}' ë²¡í„° DB ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None, None

def search_faq_in_vector_db(query_text, category_name, top_k=3):
    """ì£¼ì–´ì§„ ì¿¼ë¦¬ì— ëŒ€í•´ íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ FAQë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    if EMBEDDING_MODEL is None:
        print("ê²½ê³ : ì„ë² ë”© ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•„ FAQ ê²€ìƒ‰ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []

    index, faq_ids = load_vector_db_for_category(category_name)

    if index is None or faq_ids is None:
        return []

    query_embedding = EMBEDDING_MODEL.encode([query_text], convert_to_numpy=True)
    D, I = index.search(query_embedding, top_k)

    found_faqs = []
    for i, faq_idx_in_corpus in enumerate(I[0]):
        if faq_idx_in_corpus == -1:
            continue

        faq_id = faq_ids[faq_idx_in_corpus]
        try:
            faq_obj = FAQ.objects.get(id=faq_id)
            found_faqs.append({
                'faq_object': faq_obj,
                'similarity_score': D[0][i]
            })
        except FAQ.DoesNotExist:
            print(f"ê²½ê³ : ID {faq_id}ì— í•´ë‹¹í•˜ëŠ” FAQ ê°ì²´ë¥¼ DBì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    found_faqs.sort(key=lambda x: x['similarity_score']) # ê±°ë¦¬ê°€ ì§§ì€ ìˆœì„œ (ìœ ì‚¬ë„ ë†’ì€ ìˆœ)
    return found_faqs

# --- LLM ì„œë²„ í†µì‹  ê´€ë ¨ ì„¤ì • ---
LLM_SERVER_URL = "http://localhost:8001/generate/" # FastAPI LLM ì„œë²„ì˜ ì£¼ì†Œ

# --- Django Views ---

def index(request):
    """ë©”ì¸ ì±—ë´‡ UI í˜ì´ì§€ë¥¼ ë Œë”ë§í•©ë‹ˆë‹¤."""
    return render(request, 'chatbot_app/index.html')

@csrf_exempt
def chat_api(request):
    """
    ì‚¬ìš©ìì˜ ì±—ë´‡ ìš”ì²­ì„ ì²˜ë¦¬í•˜ê³  LLM ë‹µë³€ì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë°˜í™˜í•˜ëŠ” API ì—”ë“œí¬ì¸íŠ¸.
    FastAPI LLM ì„œë²„ë¡œë¶€í„° SSE ìŠ¤íŠ¸ë¦¼ì„ ë°›ì•„ í´ë¼ì´ì–¸íŠ¸ë¡œ ë‹¤ì‹œ í”„ë¡ì‹œí•©ë‹ˆë‹¤.
    """
    if request.method == 'POST':
        try:
            data = json.loads(request.body)
            user_question = data.get('question')
            service_category = data.get('category')

            if not user_question or not service_category:
                return JsonResponse({'error': 'ì§ˆë¬¸ê³¼ ì„œë¹„ìŠ¤ ì¹´í…Œê³ ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.'}, status=400)

            print(f"ì‚¬ìš©ì ì§ˆë¬¸: {user_question}, ì¹´í…Œê³ ë¦¬: {service_category}")

            # 1. RAG ìˆ˜í–‰: ì„ íƒëœ ì¹´í…Œê³ ë¦¬ì—ì„œ ê´€ë ¨ FAQ ê²€ìƒ‰
            relevant_faqs = search_faq_in_vector_db(user_question, service_category, top_k=3)

            # 2. LLM í”„ë¡¬í”„íŠ¸ ë° Few Shot êµ¬ì„± (ê°•ë ¥í•œ ì»¨í…ìŠ¤íŠ¸ ë° ì§€ì‹œ í¬í•¨)
            rag_context_text = ""
            few_shot_examples_for_ollama = []
            if relevant_faqs:
                rag_context_text += "ë‹¤ìŒì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ FAQ ë‚´ìš©ì…ë‹ˆë‹¤:\\n"
                for i, faq_data in enumerate(relevant_faqs):
                    # ê° FAQ ê°ì²´ì—ì„œ ì§ˆë¬¸ê³¼ ë‹µë³€ ì¶”ì¶œ
                    faq = faq_data['faq_object']

                    # FAQ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
                    rag_context_text += f"--- FAQ {i+1} ---\\n"
                    rag_context_text += f"ì§ˆë¬¸: {faq.question}\\n"
                    rag_context_text += f"ë‹µë³€: {faq.answer}\\n"

                    # Few Shot ì˜ˆì‹œë¡œ ì¶”ê°€
                    few_shot_examples_for_ollama.append(
                        {"role": "user", "content": faq.question},
                    )
                    few_shot_examples_for_ollama.append(
                        {"role": "assistant", "content": faq.answer},
                    )
                rag_context_text += "--------------------\\n"
            else:
                # ê´€ë ¨ FAQê°€ ì—†ì„ ë•Œ ëª…í™•íˆ "ì˜ ëª¨ë¥´ê² ì–´ìš”"ë¡œ ë‹µë³€í•˜ë„ë¡ ì§€ì‹œ
                rag_context_text += "í˜„ì¬ ì œê³µëœ FAQ ë‚´ìš©ì—ëŠ” ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ **'ì˜ ëª¨ë¥´ê² ì–´ìš”. ë” ìì„¸í•œ ì •ë³´ê°€ í•„ìš”í•˜ì‹œë©´ ìˆ˜íŒŒì ê³ ê°ì„¼í„°ë¡œ ë¬¸ì˜í•´ì£¼ì„¸ìš”. ğŸ™‡â€â™€ï¸'** ë¼ê³ ë§Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ë‚´ìš©ì€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.\\n\\n"


            # ìµœì¢… LLM í”„ë¡¬í”„íŠ¸: ì»¨í…ìŠ¤íŠ¸ì™€ ì‹¤ì œ ì‚¬ìš©ì ì§ˆë¬¸ ê²°í•©
            llm_request_payload = {
                "prompt": user_question, # ì‹¤ì œ ì‚¬ìš©ì ì§ˆë¬¸ (clean)
                "rag_context": rag_context_text, # RAGë¡œ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸ (ì˜µì…˜)
                "few_shot_examples": few_shot_examples_for_ollama # Few-Shot ì˜ˆì‹œ ë¦¬ìŠ¤íŠ¸
            }

            print("RAG ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸:")
            print(rag_context_text[:500])  # ì²˜ìŒ 500ìë§Œ ì¶œë ¥ (ë””

            print(f"LLMìœ¼ë¡œ ë³´ë‚¼ ë°ì´í„° (í”„ë¡¬í”„íŠ¸: '{llm_request_payload['prompt'][:100]}', "
                  f"Few-Shot ì˜ˆì‹œ ìˆ˜: {len(few_shot_examples_for_ollama) // 2}, "
                  f"RAG ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(rag_context_text)})...\n")

            # 3. LLM ì„œë²„ í˜¸ì¶œ ë° ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ í”„ë¡ì‹œ
            def generate_response_stream(payload):
                try:
                    # FastAPI LLM ì„œë²„ì— ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­
                    with requests.post(
                        LLM_SERVER_URL,
                        json=payload,
                        stream=True, # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ë°›ê¸° ìœ„í•´ True
                        timeout=120 # LLM ì‘ë‹µ ëŒ€ê¸° ì‹œê°„
                    ) as response:
                        response.raise_for_status() # HTTP ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ ì˜ˆì™¸ ë°œìƒ
                        for chunk in response.iter_content(chunk_size=None): # ì²­í¬ ë‹¨ìœ„ë¡œ ì½ìŒ
                            # FastAPI ì„œë²„ëŠ” ì´ë¯¸ SSE í˜•ì‹ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë³´ë‚´ê³  ìˆìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ yield
                            yield chunk # ë°›ì€ ì²­í¬ë¥¼ ë°”ë¡œ í´ë¼ì´ì–¸íŠ¸ë¡œ ì „ë‹¬

                except requests.exceptions.ConnectionError:
                    yield f"data: {json.dumps('í˜„ì¬ AI ìƒë‹´ ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.')}\\n\\n"
                    yield "event: end\\ndata: \\n\\n"
                except requests.exceptions.Timeout:
                    yield f"data: {json.dumps('AI ìƒë‹´ ì„œë²„ ì‘ë‹µ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.')}\\n\\n"
                    yield "event: end\\ndata: \\n\\n"
                except requests.exceptions.RequestException as e:
                    yield f"data: {json.dumps(f'AI ìƒë‹´ ì„œë²„ì™€ì˜ í†µì‹  ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}')}\\n\\n"
                    yield "event: end\\ndata: \\n\\n"
                except Exception as e:
                    yield f"data: {json.dumps(f'ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}')}\\n\\n"
                    yield "event: end\\ndata: \\n\\n"

            # StreamingHttpResponseë¡œ ì œë„ˆë ˆì´í„° ë°˜í™˜
            return StreamingHttpResponse(generate_response_stream(llm_request_payload), content_type="text/event-stream")

        except json.JSONDecodeError:
            return JsonResponse({'error': 'ì˜ëª»ëœ JSON í˜•ì‹ì…ë‹ˆë‹¤.'}, status=400)
        except Exception as e:
            print(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return JsonResponse({'error': f'ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}'}, status=500)
    else:
        return JsonResponse({'error': 'POST ìš”ì²­ë§Œ í—ˆìš©ë©ë‹ˆë‹¤.'}, status=405)


